{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b6a5609",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for ‚ÄúData Analyst‚Äù Job position in ‚ÄúBangalore‚Äù location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "    1. First get the webpage https://www.shine.com/\n",
    "    2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúJob title, Skills‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "    3. Then click the search button.\n",
    "    4. Then scrape the data for the first 10 jobs results you get.\n",
    "    5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1fc128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lead Data Analyst</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>ara resources private limited</td>\n",
       "      <td>4 to 9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>diraa hr services hiring for mncs</td>\n",
       "      <td>0 to 1 Yr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vacancy For Data Analyst</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>yogita staffing solution</td>\n",
       "      <td>0 to 3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clinical Data Analyst</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>techno endura</td>\n",
       "      <td>0 to 1 Yr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Modeler data</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>boyen haddin consulting and technol...</td>\n",
       "      <td>3 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Modeller</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>boyen haddin consulting and technol...</td>\n",
       "      <td>3 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Modeler Bangalore</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>boyen haddin consulting and technol...</td>\n",
       "      <td>3 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Modeler</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>boyen haddin consulting and technol...</td>\n",
       "      <td>3 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Clinical Data Analyst</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>quiscon biotech</td>\n",
       "      <td>0 to 2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst Hiring Fresher and Experience</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>kavya interprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Job Title Job Location  \\\n",
       "0                           Lead Data Analyst  [Bangalore]   \n",
       "1                                Data Analyst  [Bangalore]   \n",
       "2                    Vacancy For Data Analyst  [Bangalore]   \n",
       "3                       Clinical Data Analyst  [Bangalore]   \n",
       "4                           Data Modeler data  [Bangalore]   \n",
       "5                               Data Modeller  [Bangalore]   \n",
       "6                      Data Modeler Bangalore  [Bangalore]   \n",
       "7                                Data Modeler  [Bangalore]   \n",
       "8                       Clinical Data Analyst  [Bangalore]   \n",
       "9  Data Analyst Hiring Fresher and Experience  [Bangalore]   \n",
       "\n",
       "                             Company Name Experience Required  \n",
       "0           ara resources private limited          4 to 9 Yrs  \n",
       "1       diraa hr services hiring for mncs           0 to 1 Yr  \n",
       "2                yogita staffing solution          0 to 3 Yrs  \n",
       "3                           techno endura           0 to 1 Yr  \n",
       "4  boyen haddin consulting and technol...          3 to 6 Yrs  \n",
       "5  boyen haddin consulting and technol...          3 to 6 Yrs  \n",
       "6  boyen haddin consulting and technol...          3 to 6 Yrs  \n",
       "7  boyen haddin consulting and technol...          3 to 6 Yrs  \n",
       "8                         quiscon biotech          0 to 2 Yrs  \n",
       "9                       kavya interprises          0 to 4 Yrs  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the necessary libraries\n",
    "import selenium   \n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# setting up the browser\n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "# website url to open the page\n",
    "driver.get(\"https://www.shine.com/\") \n",
    "\n",
    "# click the search field\n",
    "search = driver.find_element(By.CLASS_NAME,\"iconH-zoom-white\") \n",
    "search.click()\n",
    "\n",
    "# delay the instructions for 1 sec, so the page can be loaded\n",
    "time.sleep(1) \n",
    "\n",
    "# search field path specified\n",
    "designation = driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[1]/div/input\")\n",
    "\n",
    "# providing the search data to the search field\n",
    "designation.send_keys('Data Analyst') \n",
    "\n",
    "#location field path specified\n",
    "location = driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]/div/input\")\n",
    "\n",
    "# providing the location\n",
    "location.send_keys('Bangalore') \n",
    "\n",
    "# search button class name specified.\n",
    "sjob = driver.find_element(By.CLASS_NAME,\"searchForm_btnWrap_advance__VYBHN\") \n",
    "\n",
    "# click the search button\n",
    "sjob.click() \n",
    "\n",
    "# variables declared \n",
    "jobtitle=[]  \n",
    "jloc = []\n",
    "company_name = []\n",
    "exp_req = []\n",
    "\n",
    "# scraping the tittle from the web page\n",
    "title = driver.find_elements(By.XPATH,'//h2[@itemprop=\"name\"]') \n",
    "\n",
    "# storing the text in a list\n",
    "for i in title:\n",
    "    jobtitle.append(i.text)  \n",
    "\n",
    "# scraping the location from the web page\n",
    "location = driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"]')\n",
    "\n",
    "# storing the location in a list\n",
    "for l in location:\n",
    "    patt = re.compile(r'[a-zA-Z]+')  # filtering the unwanted data\n",
    "    loc = patt.findall(l.text)\n",
    "    jloc.append(loc)\n",
    "    \n",
    "# scraping company name for the web page    \n",
    "cname = driver.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "\n",
    "# storing the company name in a list\n",
    "for c in cname:\n",
    "    company_name.append(c.text)\n",
    "    \n",
    "# scraping the exprience required from the web page    \n",
    "reqe = driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]') \n",
    "\n",
    "# storing exprience required in a list\n",
    "for r in reqe:\n",
    "    exp_req.append(r.text)\n",
    "    \n",
    "# closing the browser    \n",
    "driver.quit()  \n",
    "\n",
    "# creating Data Frame object and displaying the data \n",
    "df= pd.DataFrame({\"Job Title\":jobtitle[0:10],\"Job Location\":jloc[0:10],\"Company Name\":company_name[0:10],\"Experience Required\":exp_req[0:10]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb4a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a605a121",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "    1. First get the webpage https://www.shine.com/\n",
    "    2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúJob title, Skills‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the location‚Äù field.\n",
    "    3. Then click the search button.\n",
    "    4. Then scrape the data for the first 10 jobs results you get.\n",
    "    5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3766e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>acme services private limited</td>\n",
       "      <td>3 to 5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>ltimindtree limited</td>\n",
       "      <td>6 to 8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lead Data Scientist/ Principal Data Scientist</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>fractal</td>\n",
       "      <td>5 to 9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vacancy For Data Scientist Fresher and Experience</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>yogita staffing solution</td>\n",
       "      <td>0 to 3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>neostats</td>\n",
       "      <td>7 to 11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>aereo</td>\n",
       "      <td>7 to 10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>people staffing solutions</td>\n",
       "      <td>4 to 9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist Urgent Vacancy</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>yogita staffing solution</td>\n",
       "      <td>0 to 3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>diraa hr services</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Pharma Data Scientist</td>\n",
       "      <td>[Bangalore]</td>\n",
       "      <td>quiscon biotech</td>\n",
       "      <td>0 to 1 Yr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title Job Location  \\\n",
       "0                                     Data Scientist  [Bangalore]   \n",
       "1                                     Data Scientist  [Bangalore]   \n",
       "2      Lead Data Scientist/ Principal Data Scientist  [Bangalore]   \n",
       "3  Vacancy For Data Scientist Fresher and Experience  [Bangalore]   \n",
       "4                              Senior Data Scientist  [Bangalore]   \n",
       "5                                Lead Data Scientist  [Bangalore]   \n",
       "6                                     Data Scientist  [Bangalore]   \n",
       "7                      Data Scientist Urgent Vacancy  [Bangalore]   \n",
       "8                                     Data Scientist  [Bangalore]   \n",
       "9                              Pharma Data Scientist  [Bangalore]   \n",
       "\n",
       "                    Company Name Experience Required  \n",
       "0  acme services private limited          3 to 5 Yrs  \n",
       "1            ltimindtree limited          6 to 8 Yrs  \n",
       "2                        fractal          5 to 9 Yrs  \n",
       "3       yogita staffing solution          0 to 3 Yrs  \n",
       "4                       neostats         7 to 11 Yrs  \n",
       "5                          aereo         7 to 10 Yrs  \n",
       "6      people staffing solutions          4 to 9 Yrs  \n",
       "7       yogita staffing solution          0 to 3 Yrs  \n",
       "8              diraa hr services          0 to 4 Yrs  \n",
       "9                quiscon biotech           0 to 1 Yr  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the necessary libraries\n",
    "import selenium\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# defining driver and web browser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Pharsing wedsite link \n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "#searching the search button and clicking it\n",
    "search = driver.find_element(By.CLASS_NAME,\"iconH-zoom-white\")\n",
    "search.click()\n",
    "\n",
    "# delay for 1 second, so the data can be loaded\n",
    "time.sleep(1)\n",
    "\n",
    "# Entering the search catogery in search field\n",
    "designation = driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[1]/div/input\")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "# Entering location in the field\n",
    "location = driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[2]/div/input\")\n",
    "location.send_keys('Bangalore')\n",
    "\n",
    "# delay for 1 second, for data to load\n",
    "time.sleep(1)\n",
    "\n",
    "# clicking the search button\n",
    "sjob = driver.find_element(By.CLASS_NAME,\"searchForm_btnWrap_advance__VYBHN\")\n",
    "sjob.click()\n",
    "\n",
    "# defining variables \n",
    "jobtitle=[]\n",
    "jloc = []\n",
    "company_name = []\n",
    "exp_req = []\n",
    "\n",
    "# scraping job title and storing in list\n",
    "title = driver.find_elements(By.XPATH,'//h2[@itemprop=\"name\"]')\n",
    "for i in title:\n",
    "    jobtitle.append(i.text)\n",
    "    \n",
    "# scraping job location and storing in list    \n",
    "location = driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"]')\n",
    "for l in location:\n",
    "    patt = re.compile(r'[a-zA-Z]+') # filtering unwanted data\n",
    "    loc = patt.findall(l.text)\n",
    "    jloc.append(loc)\n",
    "\n",
    "# scraping company name and storing in list\n",
    "cname = driver.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "for c in cname:\n",
    "    company_name.append(c.text)\n",
    "    \n",
    "# scraping required exprience and storing in list    \n",
    "reqe = driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]') \n",
    "for r in reqe:\n",
    "    exp_req.append(r.text)\n",
    "    \n",
    "#driver.quit()\n",
    "\n",
    "# creating data frame and displaying data\n",
    "df= pd.DataFrame({\"Job Title\":jobtitle[0:10],\"Job Location\":jloc[0:10],\"Company Name\":company_name[0:10],\"Experience Required\":exp_req[0:10]})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c17f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "191ddf71",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the webpage\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required. The location filter to be used is ‚ÄúDelhi/NCR‚Äù. The salary filter to be used is ‚Äú3-6‚Äù lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "    1. first get the web page https://www.shine.com/\n",
    "    2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, and Companies‚Äù field.\n",
    "    3. Then click the search button.\n",
    "    4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "    5. Then scrape the data for the first 10 jobs results you get.\n",
    "    6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcbc0522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist Recruitment</td>\n",
       "      <td>[Saudi, Arabia]</td>\n",
       "      <td>renuka interprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist Recruitment</td>\n",
       "      <td>[Saudi, Arabia]</td>\n",
       "      <td>renuka interprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist Recruitment</td>\n",
       "      <td>[Saudi, Arabia]</td>\n",
       "      <td>renuka interprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATA SCIENTIST</td>\n",
       "      <td>[Other, Gujarat]</td>\n",
       "      <td>ironlist</td>\n",
       "      <td>2 to 6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Noida]</td>\n",
       "      <td>thescholarhat</td>\n",
       "      <td>1 to 3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>[Other, Gujarat]</td>\n",
       "      <td>coretus technologies</td>\n",
       "      <td>3 to 5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Python Data Scientist</td>\n",
       "      <td>[Noida]</td>\n",
       "      <td>buddy4study</td>\n",
       "      <td>4 to 8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior/Lead Data Scientist- Noida/Bangalore/Hy...</td>\n",
       "      <td>[Noida]</td>\n",
       "      <td>techneplus</td>\n",
       "      <td>3 to 8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>[Saudi, Arabia]</td>\n",
       "      <td>renuka interprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hiring For Data Scientist</td>\n",
       "      <td>[Saudi, Arabia]</td>\n",
       "      <td>renuka interprises</td>\n",
       "      <td>0 to 4 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title      Job Location  \\\n",
       "0                         Data Scientist Recruitment   [Saudi, Arabia]   \n",
       "1                         Data Scientist Recruitment   [Saudi, Arabia]   \n",
       "2                         Data Scientist Recruitment   [Saudi, Arabia]   \n",
       "3                                     DATA SCIENTIST  [Other, Gujarat]   \n",
       "4                                     Data Scientist           [Noida]   \n",
       "5                                     Data Scientist  [Other, Gujarat]   \n",
       "6                              Python Data Scientist           [Noida]   \n",
       "7  Senior/Lead Data Scientist- Noida/Bangalore/Hy...           [Noida]   \n",
       "8                          Hiring For Data Scientist   [Saudi, Arabia]   \n",
       "9                          Hiring For Data Scientist   [Saudi, Arabia]   \n",
       "\n",
       "           Company Name Experience Required  \n",
       "0    renuka interprises          0 to 4 Yrs  \n",
       "1    renuka interprises          0 to 4 Yrs  \n",
       "2    renuka interprises          0 to 4 Yrs  \n",
       "3              ironlist          2 to 6 Yrs  \n",
       "4         thescholarhat          1 to 3 Yrs  \n",
       "5  coretus technologies          3 to 5 Yrs  \n",
       "6           buddy4study          4 to 8 Yrs  \n",
       "7            techneplus          3 to 8 Yrs  \n",
       "8    renuka interprises          0 to 4 Yrs  \n",
       "9    renuka interprises          0 to 4 Yrs  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the necessary libraries\n",
    "import selenium\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# defining driver and web browser, pharsing the web link\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "# maximize window\n",
    "driver.maximize_window()\n",
    "\n",
    "#searching the search button and clicking it\n",
    "search = driver.find_element(By.CLASS_NAME,\"iconH-zoom-white\")\n",
    "search.click()\n",
    "\n",
    "# delay for 1 second,so the data can be loaded\n",
    "time.sleep(1)\n",
    "\n",
    "# Entering the search data in search field\n",
    "designation = driver.find_element(By.XPATH,\"/html/body/div/div[4]/div/div[2]/div[2]/div/form/div/div[1]/ul/li[1]/div/input\")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "# focus on search icon and clicking it\n",
    "sjob = driver.find_element(By.CLASS_NAME,\"searchForm_btnWrap_advance__VYBHN\")\n",
    "sjob.click()\n",
    "\n",
    "# delay for 1 second,so the data can be loaded\n",
    "time.sleep(1)\n",
    "\n",
    "# clicking the location filter\n",
    "loc_fil = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[3]/div/div[1]/div/div[2]/div/ul/li[1]/button')\n",
    "loc_fil.click()\n",
    "\n",
    "# delay for 1 second,so the data can be loaded\n",
    "time.sleep(1)\n",
    "\n",
    "# selection the location from the filter\n",
    "select_loc = driver.find_elements(By.CLASS_NAME,\"filter_filter_option_item__bvdpQ\")\n",
    "select_loc[10].click()\n",
    "select_loc[12].click()\n",
    "select_loc[30].click()\n",
    "\n",
    "# delay for 1 second,so the data can be loaded\n",
    "time.sleep(1)\n",
    "\n",
    "# click search after updating all the data\n",
    "result_loc = driver.find_element(By.XPATH,\"/html/body/div[2]/div/div/div/div[4]/button[2]\")\n",
    "result_loc.click()\n",
    "\n",
    "# delay for 2 second,so the data can be loaded\n",
    "time.sleep(2)\n",
    "\n",
    "# defining variables\n",
    "jt = []\n",
    "jl = []\n",
    "cn = []\n",
    "er = []\n",
    "\n",
    "# scraping job title, storing in list\n",
    "title = driver.find_elements(By.XPATH,'//h2[@itemprop=\"name\"]')\n",
    "for i in title:\n",
    "    jt.append(i.text)\n",
    "    \n",
    "# scraping job location, storing in list    \n",
    "location = driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_locationIcon__zrWt2\"]')\n",
    "for l in location:\n",
    "    patt = re.compile(r'[a-zA-Z]+')\n",
    "    loc = patt.findall(l.text)\n",
    "    jl.append(loc)\n",
    "    \n",
    "#scraping company name, storing in the list    \n",
    "cname = driver.find_elements(By.XPATH,'//div[@class=\"jobCard_jobCard_cName__mYnow\"]/span')\n",
    "for c in cname:\n",
    "    cn.append(c.text)\n",
    "    \n",
    "#scraping required exprience, storing in the list    \n",
    "reqe = driver.find_elements(By.XPATH,'//div[@class=\" jobCard_jobCard_lists_item__YxRkV jobCard_jobIcon__3FB1t\"]') \n",
    "for r in reqe:\n",
    "    er.append(r.text)\n",
    "\n",
    "# closing the browser    \n",
    "driver.quit()\n",
    "\n",
    "# creating data frame and displaying top 10 data\n",
    "df= pd.DataFrame({\"Job Title\":jt[0:10],\"Job Location\":jl[0:10],\"Company Name\":cn[0:10],\"Experience Required\":er[0:10]})\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df56889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1a70a2d",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "    6. Brand\n",
    "    7. Product Description\n",
    "    8. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "    1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "    2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands and more‚Äù is written and click the search icon\n",
    "    3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fc44fa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Product Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hooper</td>\n",
       "      <td>Polarized, UV Protection Wayfarer Sunglasses (47)</td>\n",
       "      <td>‚Çπ599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eyewearlabs</td>\n",
       "      <td>Polarized, UV Protection Rectangular Sunglasse...</td>\n",
       "      <td>‚Çπ2,199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>‚Çπ149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (54)</td>\n",
       "      <td>‚Çπ149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VINCENT CHASE</td>\n",
       "      <td>by Lenskart Polarized, UV Protection Wayfarer ...</td>\n",
       "      <td>‚Çπ499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Singco India</td>\n",
       "      <td>Gradient, Toughened Glass Lens, UV Protection ...</td>\n",
       "      <td>‚Çπ594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>Polarized, UV Protection Aviator Sunglasses (55)</td>\n",
       "      <td>‚Çπ559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized Rectangular Sunglasses (56)</td>\n",
       "      <td>‚Çπ1,247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Elgator</td>\n",
       "      <td>UV Protection Aviator, Wayfarer Sunglasses (55)</td>\n",
       "      <td>‚Çπ291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized Rectangular Sunglasses (60)</td>\n",
       "      <td>‚Çπ643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Product Name                                Product Description  \\\n",
       "0           Hooper  Polarized, UV Protection Wayfarer Sunglasses (47)   \n",
       "1      Eyewearlabs  Polarized, UV Protection Rectangular Sunglasse...   \n",
       "2             SRPM             UV Protection Wayfarer Sunglasses (50)   \n",
       "3        Elligator         UV Protection Retro Square Sunglasses (54)   \n",
       "4    VINCENT CHASE  by Lenskart Polarized, UV Protection Wayfarer ...   \n",
       "..             ...                                                ...   \n",
       "95    Singco India  Gradient, Toughened Glass Lens, UV Protection ...   \n",
       "96  ROZZETTA CRAFT   Polarized, UV Protection Aviator Sunglasses (55)   \n",
       "97       ROYAL SON              Polarized Rectangular Sunglasses (56)   \n",
       "98         Elgator    UV Protection Aviator, Wayfarer Sunglasses (55)   \n",
       "99       ROYAL SON              Polarized Rectangular Sunglasses (60)   \n",
       "\n",
       "   Product Price  \n",
       "0           ‚Çπ599  \n",
       "1         ‚Çπ2,199  \n",
       "2           ‚Çπ149  \n",
       "3           ‚Çπ149  \n",
       "4           ‚Çπ499  \n",
       "..           ...  \n",
       "95          ‚Çπ594  \n",
       "96          ‚Çπ559  \n",
       "97        ‚Çπ1,247  \n",
       "98          ‚Çπ291  \n",
       "99          ‚Çπ643  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the necessary libraries\n",
    "import selenium\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# function for scraping brand and storing in list\n",
    "def brand_search():\n",
    "    time.sleep(2)\n",
    "    brand=driver.find_elements(By.CLASS_NAME,\"_2WkVRV\")\n",
    "    for b in brand:\n",
    "        s_brand.append(b.text)\n",
    "\n",
    "# function for scraping brand information and storing in the list        \n",
    "def brand_information():\n",
    "    time.sleep(2)\n",
    "    info = driver.find_elements(By.CLASS_NAME,\"IRpwTa\")\n",
    "    for i in info:\n",
    "        brand_info.append(i.text)\n",
    "        \n",
    "# scraping product price and storing in list        \n",
    "def product_price():\n",
    "    time.sleep(2)\n",
    "    pprice = driver.find_elements(By.CLASS_NAME,\"_30jeq3\")\n",
    "    for p in pprice:\n",
    "        prod_price.append(p.text)\n",
    "        \n",
    "    \n",
    "# variable defined\n",
    "s_brand = []\n",
    "brand_info = []\n",
    "prod_price = []\n",
    "\n",
    "# browser defiend, pharsing the weblink \n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "# delay for 2 second,so the data can be loaded\n",
    "time.sleep(2)\n",
    "\n",
    "# Providing data to search field\n",
    "search = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/div/input')\n",
    "search.send_keys(\"sunglasses\")\n",
    "\n",
    "# clicking the search icon\n",
    "click_search = driver.find_element(By.CLASS_NAME,\"_2iLD__\")\n",
    "click_search.click()\n",
    "\n",
    "# calling the functions\n",
    "brand_search()\n",
    "brand_information()\n",
    "product_price()\n",
    "\n",
    "#brand=driver.find_elements(By.CLASS_NAME,\"_2WkVRV\")\n",
    "#time.sleep(2)\n",
    "#for b in brand:\n",
    "    #s_brand.append(b.text)\n",
    "\n",
    "# click next to go to next page    \n",
    "next_page= driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "next_page.click()\n",
    "#calling functions\n",
    "brand_search()\n",
    "brand_information()\n",
    "product_price()\n",
    "\n",
    "# moving to next page\n",
    "next_page.click()\n",
    "\n",
    "# calling functions\n",
    "brand_search()\n",
    "brand_information()\n",
    "product_price()\n",
    "\n",
    "#closing browser\n",
    "driver.quit()\n",
    "\n",
    "# creating data frame and displaying the data\n",
    "df=pd.DataFrame({\"Product Name\":s_brand[0:100],\"Product Description\":brand_info[0:100],\"Product Price\":prod_price[0:100]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f6a09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98b812ee",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb/product- reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market place=FLIPKART\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "    1. Rating\n",
    "    2. Review summary\n",
    "    3. Full review\n",
    "    4. You have to scrape this data for first 100 reviews.\n",
    "Note: All the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9569910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Rating Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>Superüî• and good performance üëå‚ù§Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>Excellent Phone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>very good camera quality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Awesome</td>\n",
       "      <td>iPhone 11 is a good phone. Not a very big diff...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Value for money üòç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Photos super</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>V Good all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>Perfect Product!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>It‚Äôs really awesome</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Rating  Rating Review Summary  \\\n",
       "S.No                                 \n",
       "0         5              Fabulous!   \n",
       "1         5              Brilliant   \n",
       "2         5              Brilliant   \n",
       "3         5              Excellent   \n",
       "4         5                Awesome   \n",
       "...     ...                    ...   \n",
       "95        5      Terrific purchase   \n",
       "96        5       Perfect product!   \n",
       "97        5       Perfect product!   \n",
       "98        5              Just wow!   \n",
       "99        5              Must buy!   \n",
       "\n",
       "                                            Full Review  \n",
       "S.No                                                     \n",
       "0                       Superüî• and good performance üëå‚ù§Ô∏è  \n",
       "1                                      Excellent Phone.  \n",
       "2                              very good camera quality  \n",
       "3                                                   NYC  \n",
       "4     iPhone 11 is a good phone. Not a very big diff...  \n",
       "...                                                 ...  \n",
       "95                                    Value for money üòç  \n",
       "96                                         Photos super  \n",
       "97                                           V Good all  \n",
       "98                                    Perfect Product!!  \n",
       "99                                  It‚Äôs really awesome  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the necessary libraries\n",
    "import selenium\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# defining url\n",
    "url='https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market place=FLIPKART'\n",
    "\n",
    "# variable defiened\n",
    "rating = []\n",
    "summary = []\n",
    "full_review = []\n",
    "counter = 0\n",
    "\n",
    "# function to scrap data (rating,summary, full review)\n",
    "def getdata():\n",
    "    prat = driver.find_elements(By.XPATH,'//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    for r in prat:\n",
    "        rating.append(r.text)\n",
    "        \n",
    "    rsumm = driver.find_elements(By.XPATH,'//p[@class=\"_2-N8zT\"]')\n",
    "    for s in rsumm:\n",
    "        summary.append(s.text)\n",
    "        \n",
    "    freview = driver.find_elements(By.XPATH,'//div[@class=\"t-ZTKy\"]')\n",
    "    for v in freview:\n",
    "        full_review.append(v.text)\n",
    "\n",
    "# defining driver and web browser\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "# delay for 1 second,so the data can be loaded\n",
    "time.sleep(1)\n",
    "\n",
    "# looping the program till the count is less than 100\n",
    "while counter <= 100:\n",
    "    next_page = driver.find_element(By.CLASS_NAME,'_1LKTO3')\n",
    "    next_page.click()\n",
    "    time.sleep(1)\n",
    "    getdata()\n",
    "    counter = len(rating)\n",
    "\n",
    "# closing the browser\n",
    "driver.quit()    \n",
    "   \n",
    "# creating data frame and displaying the data    \n",
    "df=pd.DataFrame({\"Rating\":rating[0:100],\" Rating Review Summary\":summary[0:100],\"Full Review\":full_review[0:100]})   \n",
    "\n",
    "# adding heading to index\n",
    "df.index.name = 'S.No'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b80b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7e9b76f",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for ‚Äúsneakers‚Äù in the search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "    1. Brand\n",
    "    2. Product Description\n",
    "    3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80088c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Product Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aadi</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aadi</td>\n",
       "      <td>Lightweight,Comfort,Summer,Trendy,Walking,Outd...</td>\n",
       "      <td>‚Çπ249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Platform Snake Lux Wn's Sneakers For Women</td>\n",
       "      <td>‚Çπ4,799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U.S. POLO ASSN.</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ4,799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Graviton Pro Sneakers For Men</td>\n",
       "      <td>‚Çπ3,150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Future Rider Concrete Jungle Sneakers For Men</td>\n",
       "      <td>‚Çπ3,378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>METRO</td>\n",
       "      <td>Mocassin For Men</td>\n",
       "      <td>‚Çπ1,609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Neeman's</td>\n",
       "      <td>Dress Sneakers Sneakers For Men</td>\n",
       "      <td>‚Çπ3,019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Morgan SL V1 Sneakers For Men</td>\n",
       "      <td>‚Çπ1,935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Club 5v5 SD Sneakers For Men</td>\n",
       "      <td>‚Çπ2,594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand Name                                Product Description  \\\n",
       "S.No                                                                       \n",
       "0                aadi                                   Sneakers For Men   \n",
       "1                aadi  Lightweight,Comfort,Summer,Trendy,Walking,Outd...   \n",
       "2                PUMA         Platform Snake Lux Wn's Sneakers For Women   \n",
       "3     U.S. POLO ASSN.                                   Sneakers For Men   \n",
       "4                PUMA                      Graviton Pro Sneakers For Men   \n",
       "...               ...                                                ...   \n",
       "95               PUMA      Future Rider Concrete Jungle Sneakers For Men   \n",
       "96              METRO                                   Mocassin For Men   \n",
       "97           Neeman's                    Dress Sneakers Sneakers For Men   \n",
       "98               PUMA                      Morgan SL V1 Sneakers For Men   \n",
       "99               PUMA                       Club 5v5 SD Sneakers For Men   \n",
       "\n",
       "     Product Price  \n",
       "S.No                \n",
       "0             ‚Çπ349  \n",
       "1             ‚Çπ249  \n",
       "2           ‚Çπ4,799  \n",
       "3           ‚Çπ4,799  \n",
       "4           ‚Çπ3,150  \n",
       "...            ...  \n",
       "95          ‚Çπ3,378  \n",
       "96          ‚Çπ1,609  \n",
       "97          ‚Çπ3,019  \n",
       "98          ‚Çπ1,935  \n",
       "99          ‚Çπ2,594  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the necessary libraries\n",
    "import selenium\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# defining url\n",
    "url = \"https://www.flipkart.com/\"\n",
    "\n",
    "# variable defined\n",
    "brand = []\n",
    "prod_desc =[]\n",
    "price =[]\n",
    "count = 0\n",
    "\n",
    "# function to scrap brand, store in list\n",
    "def get_brand_data():\n",
    "    time.sleep(1)\n",
    "    prod_brand = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for b in prod_brand:\n",
    "        brand.append(b.text)\n",
    "        \n",
    "# function to scrap product description, store in list\n",
    "def get_prod_desc():\n",
    "    time.sleep(1)\n",
    "    prodde = driver.find_elements(By.CLASS_NAME,'IRpwTa')\n",
    "    for p in prodde:\n",
    "        prod_desc.append(p.text)\n",
    "\n",
    "# function to scrap product price, store in list\n",
    "def get_prod_price():\n",
    "    time.sleep(1)\n",
    "    pprice = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for r in pprice:\n",
    "        price.append(r.text)\n",
    "\n",
    "# defining driver and web browser       \n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "# delay for 1 second,so the data can be loaded\n",
    "time.sleep(1)\n",
    "\n",
    "# locating the search field and entering the data\n",
    "search = driver.find_element(By.XPATH,'/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/div/input')\n",
    "search.send_keys('sneakers')\n",
    "\n",
    "#locating the search icon and clicking it\n",
    "search_click = driver.find_element(By.CLASS_NAME,'_2iLD__')\n",
    "search_click.click()\n",
    "\n",
    "driver.maximize_window()\n",
    "\n",
    "# looping the program till the count is less or equal to 100, callin function and moving to next page\n",
    "while count <= 100:\n",
    "    get_brand_data()\n",
    "    get_prod_desc()\n",
    "    get_prod_price()\n",
    "    next = driver.find_element(By.XPATH,\"/html/body/div/div/div[3]/div[1]/div[2]/div[12]/div/div/nav/a[11]\")\n",
    "    next.click()\n",
    "    count = len(brand)\n",
    "\n",
    "# closing browser    \n",
    "driver.quit()\n",
    "    \n",
    "# creating data frame and displaying the data    \n",
    "df = pd.DataFrame({\"Brand Name\":brand[0:100],\"Product Description\":prod_desc[0:100],\"Product Price\":price[0:100]})\n",
    "df.index.name = \"S.No\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08de2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73dc64ad",
   "metadata": {},
   "source": [
    "Q7: Go to webpage https://www.amazon.in/ Enter ‚ÄúLaptop‚Äù in the search field and then click the search icon. Then set CPU Type filter to ‚ÄúIntel Core i7‚Äù as shown in the below image:\n",
    "\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "    1. Title\n",
    "    2. Ratings\n",
    "    3. Price\n",
    "    \n",
    "# unable to click the processor filter - Intel Core i7, still working on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d12d18db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel Core i7\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "url = \"https://www.amazon.in/\"\n",
    "title = []\n",
    "ratings =[]\n",
    "price =[]\n",
    "laptopf = []\n",
    "# count = 0\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "search = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input')\n",
    "search.send_keys('Laptop')\n",
    "\n",
    "search_click = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search_click.click() \n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "#WebDriverWait(driver,2).until(EC.presence_of_element_located((By.XPATH,'//i[@class=\"a-icon a-icon-checkbox\"]')))\n",
    "\n",
    "#filter_laptop = driver.find_element(By.XPATH,'//li[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]')\n",
    "\n",
    "filter_laptop = driver.find_element(By.XPATH,'//span[@data-csa-c-content-id=\"p_n_feature_thirteen_browse-bin/12598163031\"]')\n",
    "time.sleep(2)\n",
    "filter_laptop.click()\n",
    "print(filter_laptop.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e28c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be1167e8",
   "metadata": {},
   "source": [
    "Q8 Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "    1. First get the webpagehttps://www.azquotes.com/\n",
    "    2. Click on Top Quotes\n",
    "    3. Than scrap \n",
    "            a) Quote \n",
    "            b) Author \n",
    "            c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f32df4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quote</th>\n",
       "      <th>Author</th>\n",
       "      <th>Type Of Quote</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S.No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The essence of strategy is choosing what not t...</td>\n",
       "      <td>Michael Porter</td>\n",
       "      <td>Essence, Deep Thought, Transcendentalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One cannot and must not try to erase the past ...</td>\n",
       "      <td>Golda Meir</td>\n",
       "      <td>Inspiration, Past, Trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patriotism means to stand by the country. It d...</td>\n",
       "      <td>Theodore Roosevelt</td>\n",
       "      <td>Country, Peace, War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Death is something inevitable. When a man has ...</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>Inspirational, Motivational, Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You have to love a nation that celebrates its ...</td>\n",
       "      <td>Erma Bombeck</td>\n",
       "      <td>4th Of July, Food, Patriotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Regret for the things we did can be tempered b...</td>\n",
       "      <td>Sydney J. Harris</td>\n",
       "      <td>Love, Inspirational, Motivational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>America... just a nation of two hundred millio...</td>\n",
       "      <td>Hunter S. Thompson</td>\n",
       "      <td>Gun, Two, Qualms About</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>For every disciplined effort there is a multip...</td>\n",
       "      <td>Jim Rohn</td>\n",
       "      <td>Inspirational, Greatness, Best Effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The spiritual journey is individual, highly pe...</td>\n",
       "      <td>Ram Dass</td>\n",
       "      <td>Spiritual, Truth, Yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>The mind is not a vessel to be filled but a fi...</td>\n",
       "      <td>Plutarch</td>\n",
       "      <td>Inspirational, Leadership, Education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Quote              Author  \\\n",
       "S.No                                                                          \n",
       "0     The essence of strategy is choosing what not t...      Michael Porter   \n",
       "1     One cannot and must not try to erase the past ...          Golda Meir   \n",
       "2     Patriotism means to stand by the country. It d...  Theodore Roosevelt   \n",
       "3     Death is something inevitable. When a man has ...      Nelson Mandela   \n",
       "4     You have to love a nation that celebrates its ...        Erma Bombeck   \n",
       "...                                                 ...                 ...   \n",
       "995   Regret for the things we did can be tempered b...    Sydney J. Harris   \n",
       "996   America... just a nation of two hundred millio...  Hunter S. Thompson   \n",
       "997   For every disciplined effort there is a multip...            Jim Rohn   \n",
       "998   The spiritual journey is individual, highly pe...            Ram Dass   \n",
       "999   The mind is not a vessel to be filled but a fi...            Plutarch   \n",
       "\n",
       "                                 Type Of Quote  \n",
       "S.No                                            \n",
       "0     Essence, Deep Thought, Transcendentalism  \n",
       "1                    Inspiration, Past, Trying  \n",
       "2                          Country, Peace, War  \n",
       "3           Inspirational, Motivational, Death  \n",
       "4                 4th Of July, Food, Patriotic  \n",
       "...                                        ...  \n",
       "995          Love, Inspirational, Motivational  \n",
       "996                     Gun, Two, Qualms About  \n",
       "997      Inspirational, Greatness, Best Effort  \n",
       "998                     Spiritual, Truth, Yoga  \n",
       "999       Inspirational, Leadership, Education  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the necessary libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "# defining weblink\n",
    "url = \"https://www.azquotes.com/\"\n",
    "\n",
    "# defining variables\n",
    "search_quotes = []\n",
    "search_author = []\n",
    "type_of_quote = []\n",
    "count = 0\n",
    "\n",
    "# defining driver and web browser\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "\n",
    "# locationg the top quotes and clicking it\n",
    "top_quotes = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a')\n",
    "top_quotes.click()\n",
    "\n",
    "# looping the program to run till the count is less than 1000, scraping the data\n",
    "while count < 1000:\n",
    "    WebDriverWait(driver,2).until(EC.presence_of_element_located((By.XPATH,'//a[@class=\"title\"]'))) # wait till XPATH found\n",
    "    quote = driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "    for q in quote:\n",
    "        search_quotes.append(q.text)\n",
    "    \n",
    "    author = driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "    for a in author:\n",
    "        search_author.append(a.text)\n",
    "    \n",
    "    quote_type = driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "    for t in quote_type:\n",
    "        type_of_quote.append(t.text)\n",
    "    \n",
    "    next_page = driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[3]/li[12]/a')\n",
    "    next_page.click()\n",
    "    count = len(search_quotes)  \n",
    "    \n",
    "# creating data frame and displaying the data    \n",
    "df = pd.DataFrame({\"Quote\":search_quotes,\"Author\":search_author,\"Type Of Quote\":type_of_quote})\n",
    "df.index.name = \"S.No\"\n",
    "\n",
    "# closing browser\n",
    "driver.quit()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a4ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44c20962",
   "metadata": {},
   "source": [
    "Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/.\n",
    "This task will be done in following steps:\n",
    "    1. First get the webpagehttps://www.jagranjosh.com/\n",
    "    2. Then You have to click on the GK option\n",
    "    3. Then click on the List of all Prime Ministers of India\n",
    "    4. Then scrap the mentioned data and make theDataFrame.\n",
    "    \n",
    " # still trying to figure out, how to get data from the table, website changed the links  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "772d54cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "url = \"https://www.jagranjosh.com/\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "driver.maximize_window()\n",
    "table_data = []\n",
    "row_data = []\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "opt_gk = driver.find_element(By.XPATH,'/html/body/div/header/nav/div/div/div[3]/ul/li[3]/a')\n",
    "opt_gk.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# WebDriverWait(driver,5).until(EC.presence_of_element_located((By.XPATH,'//*[@id=\"__next\"]/div[8]/section[5]/div[2]/ul/li[1]/article/h3/a')))\n",
    "\n",
    "#opt_pm = driver.find_element(By.XPATH,'/html/body/div[1]/div[6]/div[1]/div/div')\n",
    "#opt_pm.click()\n",
    "\n",
    "#time.sleep(3)\n",
    "#url = opt_pm.get_attribute('href')\n",
    "#df.pd.read_html(url)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# table = driver.find_element(By.XPATH,'//div[@class=\"TableData\"]/table')\n",
    "# rows = table.find_elements(By.TAG_NAME,'td')\n",
    "# for r in rows:\n",
    "#    row_data.append(r.text)\n",
    "    \n",
    "# print(row_data[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa01e0b",
   "metadata": {},
   "source": [
    "Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e. Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "    1. First get the webpage https://www.motor1.com/\n",
    "    2. Then You have to type in the search bar ‚Äô50 most expensive cars‚Äô\n",
    "    3. Then click on 50 most expensive cars in the world..\n",
    "    4. Then scrap the mentioned data and make the dataframe.\n",
    "    \n",
    "# unable to find XPATH, still working on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df41bce8",
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: unknown error: cannot determine loading status\nfrom disconnected: unable to send message to renderer\n  (Session info: chrome=120.0.6099.227)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6D79D2142+3514994]\n\t(No symbol) [0x00007FF6D75F0CE2]\n\t(No symbol) [0x00007FF6D74976AA]\n\t(No symbol) [0x00007FF6D747EB20]\n\t(No symbol) [0x00007FF6D747F7CE]\n\t(No symbol) [0x00007FF6D748DC02]\n\t(No symbol) [0x00007FF6D74A4471]\n\t(No symbol) [0x00007FF6D74A937A]\n\t(No symbol) [0x00007FF6D747FEC6]\n\t(No symbol) [0x00007FF6D74A40AD]\n\t(No symbol) [0x00007FF6D7522AEF]\n\t(No symbol) [0x00007FF6D7505D93]\n\t(No symbol) [0x00007FF6D74D4BDC]\n\t(No symbol) [0x00007FF6D74D5C64]\n\tGetHandleVerifier [0x00007FF6D79FE16B+3695259]\n\tGetHandleVerifier [0x00007FF6D7A56737+4057191]\n\tGetHandleVerifier [0x00007FF6D7A4E4E3+4023827]\n\tGetHandleVerifier [0x00007FF6D77204F9+689705]\n\t(No symbol) [0x00007FF6D75FC048]\n\t(No symbol) [0x00007FF6D75F8044]\n\t(No symbol) [0x00007FF6D75F81C9]\n\t(No symbol) [0x00007FF6D75E88C4]\n\tBaseThreadInitThunk [0x00007FF8B816257D+29]\n\tRtlUserThreadStart [0x00007FF8B9C0AA58+40]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m     11\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome()\n\u001b[1;32m---> 12\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.motor1.com/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m search_opt \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/html/body/div[10]/div[2]/div/div/div[3]/div/div/div/form/input\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m search_opt\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m50 most expensive cars\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:356\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 356\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mWebDriverException\u001b[0m: Message: unknown error: cannot determine loading status\nfrom disconnected: unable to send message to renderer\n  (Session info: chrome=120.0.6099.227)\nStacktrace:\n\tGetHandleVerifier [0x00007FF6D79D2142+3514994]\n\t(No symbol) [0x00007FF6D75F0CE2]\n\t(No symbol) [0x00007FF6D74976AA]\n\t(No symbol) [0x00007FF6D747EB20]\n\t(No symbol) [0x00007FF6D747F7CE]\n\t(No symbol) [0x00007FF6D748DC02]\n\t(No symbol) [0x00007FF6D74A4471]\n\t(No symbol) [0x00007FF6D74A937A]\n\t(No symbol) [0x00007FF6D747FEC6]\n\t(No symbol) [0x00007FF6D74A40AD]\n\t(No symbol) [0x00007FF6D7522AEF]\n\t(No symbol) [0x00007FF6D7505D93]\n\t(No symbol) [0x00007FF6D74D4BDC]\n\t(No symbol) [0x00007FF6D74D5C64]\n\tGetHandleVerifier [0x00007FF6D79FE16B+3695259]\n\tGetHandleVerifier [0x00007FF6D7A56737+4057191]\n\tGetHandleVerifier [0x00007FF6D7A4E4E3+4023827]\n\tGetHandleVerifier [0x00007FF6D77204F9+689705]\n\t(No symbol) [0x00007FF6D75FC048]\n\t(No symbol) [0x00007FF6D75F8044]\n\t(No symbol) [0x00007FF6D75F81C9]\n\t(No symbol) [0x00007FF6D75E88C4]\n\tBaseThreadInitThunk [0x00007FF8B816257D+29]\n\tRtlUserThreadStart [0x00007FF8B9C0AA58+40]\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.motor1.com/')\n",
    "\n",
    "search_opt = driver.find_element(By.XPATH,'/html/body/div[10]/div[2]/div/div/div[3]/div/div/div/form/input')\n",
    "search_opt.send_keys('50 most expensive cars')\n",
    "\n",
    "# click_opt = driver.find_element(By.XPATH,'//button[@class=\"m1-search-form-button-animate icon-search-svg m1-mobile-search\"]')\n",
    "# click_opt.click()\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774adf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
